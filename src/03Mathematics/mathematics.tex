\graphicspath{{../03Mathematics/pics/}}

\chapter{Mathematics}\label{ch:Mathematics}

\lettrine[lines=2]{\color{darkocre}M}{athematical concepts and tools used in quantum theory are} not significantly different from the ones used in classical physics. This fact, of course, does not make them any easier, but it is important to remember that there is no special mathematics that makes quantum physics extra challenging.

In this chapter we develop mathematical tools needed for quantum theory. This is an essential investment that will pay off when we start discussing quantum phenomena. Mathematics plays a vital role in quantum physics since it becomes the only guide in the field where intuition and simple mental pictures fail.

\begin{myprereq}{Prerequisite Knowledge}
	To fully understand the material of this chapter, readers should be comfortable with the following concepts:
	
	\begin{itemize}
		\item \phantom{phantom}
		\vspace{-0.5cm}
		\item State
		\item Dynamical equations
	\end{itemize}	
\end{myprereq}


\section{Randomness}
Randomness is the opposite of certainty, \emph{determinism}, and complete predictability. Randomness (or \emph{indeterminism}) plays an essential part in quantum physics.

Mathematical description of randomness is based on the idea of \emph{probability}. Probability quantifies (measures) the qualitative notion of \emph{likelyhood} of certain outcomes. A common example is the toss of a coin where the probability of tail is $1/2$, or the throw of a dice, where the probability of getting a face value divisable by 3 is $1/3$.

In quantum physics probability of an event $E$ happening is understood as the \emph{relative frequency} or ratio of the number of occurences of the given event in $N$ trials:
\[
P_E = \frac{N_E}{N}\,,
\]
where $P_E$ is the probability of the event $E$, and $N_E$ is the number of times the event happens during an experiment.

For example, suppose  we are measuring a week light with a detector capable of detecting  single photons. An arriving photon triggers the detector which produces a pulsed electronic signal, like a spike in the output voltage.

Denoting $N$ the number of times the detector is triggered during an experiment. If $N_A$ is the number the detector $A$ is fired, then the probability of detecting a signal there is 
\[
P_A = \frac{N_A}{N}\,.
\]
?$N_A + N_B$ if both are working?


\subsection{Schrodinger Student}
In 1935 paper discussing quantum features, such as \emph{entanglement}, Erwin Schrodinger introduced his famous cat. He also discussed an example of students.


\section{Functions}

The idea of a function is a very basic one. Essentially, function is an unambiguous \emph{rule}, an \emph{algorithm}, which associates a certain value $y$ (\emph{result} or \emph{output} of a function) with every meaningful \emph{input} value $x$ (\emph{argument} of a function). As an example, consider the following  function:
\[
y=\frac{1}{2x^2+5}\,.
\]
For any real number $x$ we can compute the value $y$ using only basic arithmetic operations. 

Next consider a function $\btc{sqrt}$ which computes a square root of a number: $y=\btc{sqrt}\,x$. If we only work with real numbers, the range of meaningful inputs is reduced -- only non-negative input values $x$ are allowed. Another thing to notice is that now the function is simply given a name  $\btc{sqrt}$  and shows no structure -- it is not expressed in terms of other more basic operations. A few important functions are given names: 
$\sin\,,\cos\,,\exp\,,\log\,,\btc{abs}$\footnote{Absolute value of a number -- its positive magnitude.} and several more. Of course, the square root of a number is traditionally written with a special sign -- called a \emph{surd}: $\btc{sqrt}\,x=\sqrt{x}$.

\subsection{Function Boxes}
The input-output view of functions leads to a helpful picture where a given function is represented as a box with an input and an output (or multiple inputs and even multiple outputs), as illustrated in Figure \ref{fig:functionAsBox}.
\begin{figure}[htbp]
	\centering
	\includegraphics[scale=1.0]{functionAsBox}
	\caption{A function can be viewed as a box with input(s) and output(s).}
	\label{fig:functionAsBox}
\end{figure}


\begin{figure}[htbp]
	\centering
	\includegraphics[scale=1.0]{functionTypes}
	\caption{Several types of functions: (1); (2); (3); (4); (5).}
	\label{fig:functionTypes}
\end{figure}

\subsection{Application Notation}
The dominant rule for writing function $f$ applied to an argument $x$ uses parentheses around the argument, like so: $f(x)$. This rule, however, is \emph{not absolute} and is abandoned as soon as one uses functions in linear algebra. For example, when an operator $\op{f}$ is applied to a vector $\ket{x}$, we would write $\op{f}\,\ket{x}$, without the parentheses. 

In this book we will use a uniform rule for function application: \emph{Function and its argument are separated by a space. Only structured arguments are surrounded by parentheses. Simple arguments are written without parentheses.} Thus, we will always write
\[
\sin\,x\,,\quad \exp\, x\,,\quad \textrm{abs}\,x\,, 
\]
and so on. We can even write -- without any confusion -- expressions like
\[
\cos\frac{\phi}{3}\,,\quad\log\sqrt{y}\,.
\]
However, we will always use parentheses in the expressions similar to
\[
	\tan (\alpha+\beta)\,,
\]
to avoid confusion with another valid expression: $\tan \alpha+\beta=(\tan \alpha)+\beta$.

\subsection{Multi-Input Functions}
Function of a single argument are the most familiar kind. However, functions with two inputs are also widely used. The simplest example is the function of two arguments (\emph{binary function}):
\[
\btc{add}\,x\,y = x + y\,.
\]
On the left-hand side the function is written using so-called \emph{prefix notation}, where the name precedes the arguments. On the right-hand side the same function is written using a more conventional \emph{infix notation}, in which a special symbol is placed between two arguments. Other examples if binary functions can be given:
\[
\btc{mul}\,x\,y = x * y=xy\,,
\]
\[
\btc{pow}\,x\,n = x^\wedge n =x^n\,,
\]
\[
\btc{max}\,x\,y = x\,\textrm{if}\,x>y,\,\textrm{otherwise}\,y\,.
\]
Note that the infix notation works only for binary functions and special symbols (like "$+$") exist only for small number of them. In short, infix notation, although convenient, lacks generality needed for more powerful and abstract mathematics.

Any formula expressing a physical quantity can be viewed as a function with multiple inputs. Consider Newton's law for gravitational attraction between two point-like bodies. The force is given by
\[
F = G\frac{Mm}{r^2}\,.
\]
Assuming that the gravitational constant $G$ is a fixed number, the expression for the force depends on three arguments -- two masses and the distance between them. 
\begin{exercise}
	Consider a \emph{ternary function} (function of three arguments):
	\[
	f\,M\,m\,r=Mm/r^2\,.
	\]
	Express it purely in terms of the binary function \btc{mul} and the unary function \btc{inv}.
	\label{exe:ternaryFunction}
\end{exercise}

\subsection{Partial Application}
The box-view of functions leads to a simple, yet powerful, concept of a \emph{partially applied} function. A function of multiple arguments is called partially applied if not all its "inputs" are "filled" (i.e. assigned fixed values). 

Consider, for instance, the ternary function from the Exercsie \ref{exe:ternaryFunction}. Suppose we study how two specific bodies,with masses $M=10$ and $m=1$, interact gravitationally . Then the force $F$ between these bodies is a unary function $F_r$ of the distance $r$:
\[
F_r = G*(f\,10\,1\,r) = 10G/r^2\,.
\]
Here we partially applied the ternary function $f$ to only two arguments, leaving the third argument $r$ a free parameter. As the result of such partial application we obtained a unary function of the distance $r$ between two boides.

Let's consider another example of partial application. This illustration might seem trivial, but it will help understand the role of partial application in the case of \emph{dual} objects, such as ket and bra vectors of quantum theory. First, we write a product of two numbers using prefix notation: $\btc{mul}\,x\,y$. Then, partially apply the binary function \btc{mul} to some number, say $3$. This results in a unary function \btc{trp} which simply triples the value of its argument:
\[
f_y = \btc{trp}\,y=3y\,.
\] 

\subsection{Linearity}
Some functions are simpler than others. For example, a \emph{symmetric} function has the same value for $x$ and $-x$: $f\,x=f\,(-x)$. A general function does not have such a property, and in this sense symmetric functions are simpler than a general function.

Among the simplest kinds of functions, \emph{linear functions} are of special importance. Such functions have the following properties:
\[
f\,(x+y) = (f\, x)+(f\,y)\quad\textrm{and}\quad f\,(ax)=a(f\, x)\,.
\]
These requirements are called \emph{linearity conditions}.

\begin{exercise}
	Check whether the function \btc{trp} is a linear function.
\end{exercise}

For numeric functions, the linearity conditions are very restricting. Linear numeric functions all have the same form:
\[
f_a\,x=a*x\,,
\]
for some number $a$. Thus, for each number $a$ there corresponds a linear numeric function $f_a$ and its action on any argument $x$ is a simple multiplication by the number $a$.

The simuation becomes less trivial when we consider linear functions whose arguments are not simple numbers (e.g. vectors, operators, or even functions).


\section{Numberlikes}
Physics without numbers is unimaginable. But simple numbers, like \emph{natural numbers} 1, 2, 3, and so on are very limiting.  As the range of application of mathematics increased, numbers evolved from natural numbers, to \emph{whole} numbers, to \emph{fractions}, to \emph{real} numbers, and then to \emph{complex} numbers and to \emph{quaternions}\footnote{Octanions are not used in physics widely enough to be discussed here.}. The concept of a number became increasingly less intuitive, more abstract and powerful.

Today mathematics offers several \emph{mathematical objects} which behave essentially like numbers, but which also allow more powerful manipulations and thus can be used in wider range of problems. Examples of such \emph{numberlikes} are \emph{vectors}, \emph{tensors}, and \emph{operators}. We will explore all these objects in this chapter and will see that these three concepts are actually closely related to each other (e.g. vectors are tensors, and tensors are operators!)


\begin{flushleft}
	{\it Addition}
\end{flushleft}
The essential characteristic of numbers is the ability to \emph{add} two of them to get another number:
\[
x\,\boxplus\, y = z\,.
\]
The addition operation satisfies two simple requirements
\[
x\,\boxplus\, y = y\,\boxplus\, x\quad\textrm{ -- commutativity}\,,
\]
and
\[
(x\,\boxplus\, y)\,\boxplus\, z = x\,\boxplus\, (y\,\boxplus\, z) \quad\textrm{ -- associativity}\,.
\]


Additivity is "contagious" -- it propagates to other mathematical objects which operate on "usual" numbers. For example, it is easy to give a constructive meaning to the following expression:
\[
f = \sin\boxplus\exp\,.
\]
Here we add two numeric functions to create a new function $f$. To describe this function we must specify what it does to all possible arguments. In this case it is simply
\[
f\,x=(\sin\, x)+(\exp\, x)\,.
\]
Thus, the ability to add numbers leads to the ability to \emph{add functions}. It must be emphasized, that in the expressions like $\sin\boxplus\exp$ we are not adding numerical values of the functions, we are adding functions -- completely different mathematical objects. Adding functions becomes very useful for certain function types called \emph{operators}, as explained in section \ref{sec:operators}.

\begin{flushleft}
	{\it Multiplication}
\end{flushleft}
Repeated addition of numbers leads to the idea of multiplication. For "normal" numbers mutiplication has two properties analogous to addition:
\[
x\,*\, y = y\,*\, x\quad\textrm{ -- commutativity}\,,
\]
and
\[
(x\,*\, y)\,*\, z = x\,*\, (y\,*\, z) \quad\textrm{ -- associativity}\,.
\]
Furthermore, mutiplication and addition possess \emph{distributivity}:
\[
x\,*\, (y\,+\,z)=x\,*\, y\,+x\,*\, z\,.
\]

The ability to add "normal" numbers allows one to introduce \emph{multiplication of functions}. Indeed, we can give a constructive meaning to an expression 
\[
f = \sin\boxtimes\exp\,.
\]
It must be emphasized again: this is not a multiplication of the numeric values of the functions $\sin$ and $\exp$, it is the mutliplication of the functions themselves. 
To find the value of a unary function $f$, we simply write
\[
f\,x=(\sin\,x)*(\exp\,x)=y\,*\, z\,.
\]
Now on both sides of this equality we have "normal" numbers: On the left-hand side we have the value $f\,x$ of a unary function $f$ applied to the numeric argument $x$, on the right-hand side we have to numeric values $y=\sin\,x$ and $z=\exp\,x$ multiplied in a "usual" way.

The example given above can be generalized to an arbitrary pair of unary numeric functions $f$ and $h$. It is not difficult to convince yourself that the "product" $f\,\boxtimes\, h$ is commutative, associative, and is also distributive with respect to the "addition":
\[
f\,\boxtimes\, (h\,\boxplus\, g) = (f\,\boxtimes\, h)\,\boxplus\,(f\,\boxtimes\, g)
\]
for three unary numeric functions $f$, $h$, and $g$. In other words, unary numeric functions can be made to behave like numbers. One can view functions and manipulate them as objects on their own, without referring to their arguments.
\begin{mybio}{Point-free Notation}
	Manipulating functions without explicitely writing their arguments is known as \emph{argument-free notation} or \emph{point-free notation}. It is a useful practice and is common in quantum theory.
\end{mybio}

\begin{flushleft}
	{\it Composition}
\end{flushleft}
Functions, and their "brothers" operators, allow an additional way to combine two function in order to create another one. It is called \emph{composition} or, sometimes, \emph{sequencing} of two functions. Let's illustrate the idea using the familiar functions $\sin$ and $\exp$. We can "create" (define) a function 
$f$ which acts on its input argument in the following way:
\[
f\,x=\sin\,(\exp\, x)\,.
\]
We first apply the function $\exp$ to the input argument $x$ to obtain a numeric value $y=\exp\, x$, and then apply the function $\sin$ to $y$. We applied two functions in sequence. A special notation exists for composition. We write $f = \sin\circ\exp$. Here is used an argument free notation, writing simply $f$ instead of "$f$ of $x$", similar how we write numbers simply as $n$ instread of "$n$ apples".

Unlike addition and multiplication, \emph{composition is not commutative}:
\[
\sin\circ\exp\ne\exp\circ\sin\quad\textrm{because}\quad\sin\,(\exp\, x) \ne \exp\,(\sin\, x)\,.
\]
However, \emph{composition is associative.} Given three functions $f$, $h$, and $g$ we can combined them in two different orders, specified by the parentheses:
\[
(f\circ h)\circ g = f\circ(h\circ g)\,.
\]
Both sides of this equality represent the same value $f\,(h\, (g\, x))$: We first evaluate $y=g\, x$, then feed it into $h$ to find $z=h\,y$, and finally input it as the argument to $f$.

\begin{exercise}
	Check whether composition is distributive with respect to an "addition" of functions.
\end{exercise}

\begin{mybio}{Composition In Quantum Physics}
	Composition is an very powerful way of creating new functions by combining a given pair of functions. Composition has special significance for \emph{linear operators} -- linear functions operating on non-numeric arguments (e.g. on vectors). 
	
	In quantum theory different operators represent different measurement operations, such as  measurement of position, momentum, energy, angular momentum, spin, and so on. Only \emph{linear operators} are used in quantum theory. These operators act on special vectors, as we will later learn.
	
	Like any two functions, two quantum operators $\op{A}$ and $\op{B}$ can be composed:
	\[
	\op{C} = \op{A}\circ\op{B}\,.
	\]
	It is customary to drop the infix composition sign and simply write $\op{C} = \op{A}\op{B}$.
	
	An operator can be composed with itself. In this case a special notation exists to avoid long and clumsy expressions:
	\[
		\op{A}\circ\op{A}=\op{A}\op{A}=\op{A}^2\,.
	\]
	In general, the expression $\op{A}^n$ represents the operator $\op{A}$ composed with itself $n$ times.
	
\end{mybio}

\section{Kalcoolus}
Quantum theory uses many mathematical tools, including linear algebra and calculus. In this book the full power of the latter won't be need. Instead, we will use "kids menu calculus" or "kalcoolus" -- a small set of simple calculus-related tools sufficient to express most of the equations of quantum theory. We will begin by discussing kalcoolus variants of \emph{derivative} and \emph{integral}.

\subsection{$\Delta-\delta-\partial$ Notation}
Imagine we are tracking time from the moment we wake up at 6 a.m. to the moment we go to bed 16 hours later. Every event $E$ during the day is assigned a certain time $t_E$. We can ask how much time elapsed between two events. If the first event happens right after we wake up and the second at noon, then we write the time interval as
\[
\Delta\, t = t_2 - t_1\,.
\]
This is an example of $\Delta$-notation for \emph{sizable change} of any variable, in this case it is time.

Now consider an eye-blink, where we close the eyes at $t_1$ and open them at $t_2$. In this case we write
\[
\delta\, t = t_2 - t_1\,,
\]
using $\delta$-notation to emphasize that the time interval is \emph{tiny}.

Suppose we are monitoring the temperature outside by looking at a thermometer. For each moment of time $t$ we can specify the measured temperature $f_t$. The temperature in the morning and the temperature at noon can be quite different, so we use $\Delta$-notation:
\[
\Delta f=f_{t_2} - f_{t_1}=f(t_2)-f(t_1)=f(t_1+\Delta t)-f(t_1)\,.
\]
For the tiny time interval the change of temperature is also expected to be tiny. Physically this means that there are no extreme events causing rapid jumps in temperature. Mathematically, this means that the function $f$ is \emph{continuous}. In such cases we use $\delta$-notation:
\[
\delta f=f(t_1+\delta t)-f(t_1)\,.
\]
Such relation can be written for any moment of time $t$:
\[
\delta f=f(t+\delta t)-f(t)\,.
\]

Often it is important to know \emph{how quickly} a value is changing. For example, we can speak of the  \emph{rate of change} of temperature with respect to time:
\[
\frac{\delta f}{\delta t} = \frac{f(t+\delta t)-f(t)}{\delta t}\,.
\]
Instead of the fractions, we will use $\partial$-notation:
\[
\partial_t\,f=\frac{\delta f}{\delta t}\,.
\]

The notation just introduced can be applied to \emph{any} function of \emph{any} variable or \emph{any} number of variables. It is general, relatively simple, and efficient.
\begin{exercise}
	Given the kinetic energy of a moving body is $E = mv^2/2$, write out fully the meaning of the following expressions: $\partial_m\, E$ and $\partial_v\, E$.
\end{exercise}

\begin{mybio}{Derivative}
	The rigorous mathematical notion of derivative is used when we want to be absolutely sure that we are dealing with a unique value for the rate of change. To arrive at this unique value, one would analyze the result of making the change $\delta x$ of the variable $x$ gradually ever smaller, and prove that the ratio
	\[
		\frac{f(x+\delta x)-f(x)}{\delta x}
	\] 
	is approaching a unique value.
	
	For simplicity and practical purposes, we will use the rate of change $\partial_x f$, remembering that
	\[
	\textrm{derivative} = \partial_x f + \textrm{small error}\,.
	\]
\end{mybio}

\begin{mybio}{Full Differential}
	Readers familiar with calculus might be wondering why don't we write $df$ instead of $\delta f$.
\end{mybio}

\subsection{Bernoulli Sums}
A Swiss mathematician Jacob Bernoulli, who worked at the end of the 17th century, made many  important contributions to mathematics. In his 1713 book "The Art of Conjecturing" he presented formulas for the "sum of powers":
\[
\int n = \frac{1}{2}nn+\frac{1}{2}n\,,
\]
\[
\int nn = \frac{1}{3}n^3+\frac{1}{2}nn+\frac{1}{6}n\,,
\]
and so on up to the power of $n^{10}$, and gave a general expression for $\int n^c$.  

The summation sign used by Bernoulli was introduced by his contemporary --  a German mathematician Gottfried Leibniz. The sign is just the first letter of the word \emph{'Sum'}, written in accordance with the rules of that time. For example, a 1661 astronomy reference book {\it "Astronomia Carolina"}, contains a section titled {\it Of the $\int\textrm{\!\!\!econd}$ Inequality of the Moon}.

History aside, notation is a matter of convention. In this book we will use Leibniz summation sign for "regular" sums, like Bernoulli, as well as for "special" sums used to express \emph{integrals}.

\begin{myExample}
	Using Leibniz summation sign, we can say that "sizable change is the sum of many tiny changes" by writing the following expression:
	\[
	\Delta x = \int \delta x
	\]
	The right-hand side stands for a long expression $\delta x + \delta x + \ldots + \delta x$.
	
\end{myExample}

\section{Arrows}
Before we introduce the concepts of \emph{vectors}, \emph{vector spaces}, and \emph{operators} acting on vectors, we will examine an introductory model for vectors based on \emph{arrows} -- directed line segments, as illustrated in Figure \ref{fig:arrowsAndVectors}.


\begin{figure}[htbp]
  \centering
  \includegraphics[scale=1.0]{arrowsAndVectors}
  \caption{Arrows provide a simple geometric example of vector quantities. The idea of vectors, however, is more powerful and extends beyond this simple representation as directed line segments.}
  \label{fig:arrowsAndVectors}
\end{figure}

Symbolically, we will denote vectors by placing an arrow over letters:
\[
\vec{a}\,,\vec{b}\,,\vec{c}\,,\ldots\,,\vec{\alpha}\,,\vec{\beta}\,.
\]
However, not all vectors behave like arrows. Futhermore, to introduce the basic notation of quantum theory as early as possible, we will switch to what is known as \emph{Dirac notation} for vectors.

\subsection{Dirac Notation}
Instead of placing an arrow on top of a letter, we enclose the letter between a pair of symbols as follows:
\[
\vec{a}\to\ket{a}\,.
\]
The advantages of this notation accumulate and become more apparent the longer we study quantum physics.

\subsection{Arrow Algebra}
Arrows are \emph{numberlike} -- they can be added and multiplied, and the rules for addition and multiplication resemble those of regular numbers. However, arrows allow a richer set of operations compared to numbers. For example, there are three different types of multiplication for any pair of arrows, depending on the type of the final result: 1) a number; 2) another arrow; 3) more complex geometric figures like a piece of a plane. We will mainly focus on the first type, called \emph{scalar product}, and the thrid one, called \emph{tensor product}.

\subsection{Bases}
Consider a pair of non-parallel arrows $\ket{a}$ and $\ket{b}$, arranged for convenience tail-to-tail. Their sum yields the third arrow $\ket{c}$:
\[
\ket{c} = \ket{a} + \ket{b} = 1\ket{a}+1\ket{b}\,.
\]
The numbers in front of $\ket{a}$ and $\ket{b}$ are called \emph{components} of the arrow $\ket{c}$ \emph{relative} to $\ket{a}$ and $\ket{b}$. By varying these numbers, while keeping the arrows $\ket{a}$ and $\ket{b}$ fixed, we can \emph{obtain any arrow in a plane}. In other words, any arrow $\ket{v}$ can be written as
\[
\ket{v} = v_a\ket{a}+v_b\ket{b}\,.
\]
Here, again, the pair of numbers $(v_a, v_b)$ represent components of $\ket{v}$ relative to $\ket{a}$ and $\ket{b}$. 

Non-parallel arrows $\ket{a}$ and $\ket{b}$ are \emph{independent}, in the sense that neither can be expressed in terms of the other. In contrast, the arrow $\ket{v} = v_a\ket{a}+v_b\ket{b}$ is not independent from $\ket{a}$ and $\ket{b}$. Thus, the set of arrows $\lbrace \ket{a}, \ket{b}, \ket{v} \rbrace$ is not independent. For arrows in a plane, the number of independent arrows can not be more than two -- matching the dimensionality of the plane.

The set of $n$ independent arrows is called \emph{basis} for the $n$-dimensional space in consideration. Any set of $m < n$ independent arrows will be \emph{incomplete} and insufficient to serve as a basis. In essense, basis is a set of "building blocks" for all possible arrows. The set of "building blocks" must be rich enough to build up anything else (\emph{complete set}), and it does not need any redundant elements (\emph{independent set}).

Basis in not unique, as we can easily show. Given basis $\ket{a}$ and $\ket{b}$, we can  rotate and scale each arrow separately, to obtain arrows $\ket{\alpha}$ and $\ket{\beta}$:
\[
\ket{a}\to\ket{\alpha}\,\textrm{ and }\,\ket{b}\to\ket{\beta}\,.
\]
As long as $\ket{\alpha}\nparallel\ket{\beta}$, they can be used as basis.
\begin{exercise}
	Consider $\ket{+} = \ket{a} + \ket{b}\,\textrm{ and }\,\ket{-}=\ket{a}-\ket{b}\,.$
	Prove that if $\ket{+}=\lambda\ket{-}$ then $\ket{b}=\mu\ket{a}$. Find $\mu$.
\end{exercise}
As the previous exercise shows, the arrows $\ket{+}$ and $\ket{-}$ are independent and can be used as basis.
\begin{exercise}
	Given $\ket{v}=\ket{a}+2\ket{b}$, express it in the $\lbrace \ket{+},\ket{-}\rbrace$ basis:
	\[
		\ket{v}=v_{+}\ket{+}+v_{-}\ket{-}\,.
	\]
	Find $v_{+}$ and $v_{-}$.
\end{exercise}

\subsection{Normalized Bases}
Vectors are sometimes defined as mathematical quantities with \emph{magnitude} and \emph{direction}. This implies that a meaningful notion of \emph{length} (magnitude) can be assinged to vectors. For arrows this is obvious, while for other types of vector-like quantities might be not so. 

The length of a vector is called its \emph{norm}. Vectors with unit length are called \emph{normalized}. Any vector can be "scaled" to have a unit length. Indeed, given a vector $\ket{a}$ with length $a$, we get a normalized vector as follows:
\[
\ket{u} = \frac{\ket{a}}{a}\,.
\]
When doing calculations, it is convenient to normalize all basis vectors. This results in \emph{normalized basis}.

\section{Scalar Product}
The first way to multiply two vectors that we will study is called \emph{scalar product}. In this operation two vectors are combined to yield a number (a.k.a \emph{scalar}):
\[
\ket{a}\cdot\ket{b} = x\,.
\]
Here we followed traditional \emph{infix notation} using "$\cdot$" as an analogue of "$*$" for numbers. Soon we will learn the usefulness of \emph{prefix notation} for scalar product and its relation to operators, but for now we will keep writing scalar product of vectors similar to numbers.

Guided by simplicity, we expect scalar product to satisfy two basic algebraic laws:
\[
\ket{a}\cdot\ket{b}  = \ket{b}\cdot\ket{a}\,\textrm{  -- commutativity}\,. 
\]
\[
(\ket{a}+\ket{b})\cdot\ket{c}=\ket{a}\cdot\ket{c}+\ket{b}\cdot\ket{c}\,\textrm{ -- distributivity }.
\]
Let's apply the last requirement to a vector $2\ket{a}$, representing it as $\ket{a}+\ket{a}$:
\[
(2\ket{a})\cdot\ket{b}=(\ket{a}+\ket{a})\cdot\ket{b}=\ket{a}\cdot\ket{b}+\ket{a}\cdot\ket{b}=2(\ket{a}\cdot\ket{b})\,.
\]
Thus, we see that one more requirement is both natural and helpful:
\[
(x\ket{a})\cdot\ket{b}=x(\ket{a}\cdot\ket{b})\,.
\]
We can formulate this last result as a rule: "scalars can be pulled outside."

\subsection{Products of Basis Vectors}
Combining scalar product with the representation of vectors in a \emph{normalized} basis $\lbrace 
\ket{u_1}, \ket{u_2}\rbrace$ leads to important insights. Consider two vectors:
\[
\ket{a}=a_1\ket{u_1}+a_2\ket{u_2}\,\textrm{ and }\,\ket{b}=b_1\ket{u_1}+b_2\ket{u_2}\,.
\]
Scalar product $\ket{a}\cdot\ket{b}$ can be calculated step-by-step, first expanding $\ket{a}$:
\[
\ket{a}\cdot\ket{b} = a_1\ket{u_1}\cdot\ket{b}+a_2\ket{u_2}\cdot\ket{b}\,.
\]
Then, expanding $\ket{b}$, we obtain:
\[
\ket{a}\cdot\ket{b}=a_1b_1\ket{u_1}\cdot\ket{u_1}+a_1b_2\ket{u_1}\cdot\ket{u_2}+a_2b_1\ket{u_2}\cdot\ket{u_1}+a_2b_2\ket{u_2}\cdot\ket{u_2}\,.
\]
Using the commutativity $\ket{u_1}\cdot\ket{u_2}=\ket{u_2}\cdot\ket{u_1}$, we can slightly simplify the last expression:
\[
\ket{a}\cdot\ket{b}=a_1b_1\ket{u_1}\cdot\ket{u_1}+(a_1b_2+a_2b_1)\ket{u_1}\cdot\ket{u_2}+a_2b_2\ket{u_2}\cdot\ket{u_2}\,.
\]
If the scalar products of all basis vectors are known, we can calculate scalar product of any pair of vectors. We reduced the problem to defining the scalar product of a pair of unit-length vectors:
\[
\ket{u_1}\cdot\ket{u_1}\,,\ket{u_2}\cdot\ket{u_2}\,,\textrm{ and }\ket{u_1}\cdot\ket{u_2}\,.
\]

The only difference between two unit-length vectors $\ket{u_1}$ and $\ket{u_2}$ is their orientation. Since there is no preferred direction in the plane, we can only speak of the \emph{mutual orientation} of the pair of vectors. A convenient measure of the mutual orientation is the \emph{angle between} corresponding arrows. We are, therefore, led to the following candidate for the scalar product:
\[
\ket{u_1}\cdot\ket{u_2} = f_\theta\,,
\]
where $\theta$ is the angle between $\ket{u_1}$ and $\ket{u_2}$, measured counterclockwise from $\ket{u_1}$ to $\ket{u_2}$. At this point, the only thing we can say about the function $f$ is that it is  continuous and periodic in its argument $\theta$. Let's examine two simple candidates -- trigonometric functions $\cos\theta$ and $\sin\theta$.

\subsection{Scalar Product Meaning}
Once the scalar product of two unit vectors is defined, we can ask what intuitive interpretation can be given to a scalar product in general. To this end, let's take a pair of normalized vectors $\ket{u_1}$ and $\ket{u_2}$ and scale them to an arbitrary lengths:
\[
\ket{u_1}\,\to\,\ket{a}=a\ket{u_1}\,\textrm{ and }\,\ket{u_2}\,\to\,\ket{b}=b\ket{u_2}\,.
\]
Scalar product of $\ket{a}$ and $\ket{b}$ is then simply
\[
\ket{a}\cdot\ket{b} = abf_\theta\,.
\]
The factor $ab$ -- the product of two lengths -- suggests that the scalar product is related to area, as illustrated in Figure \ref{fig:scalarProductMeaning}. 
\begin{figure}[htbp]
	\centering
	\includegraphics[scale=1.0]{scalarProductMeaning}
	\caption{Two way to interpret scalar product of two vectors.}
	\label{fig:scalarProductMeaning}
\end{figure}

If we note that the vector $\ket{b}$ can be written as the sum of two parts:
\[
\ket{b}=\ket{b_\parallel}+\ket{b_\perp}\,,
\]
 one parallel to the vector $\ket{a}$ and the other perpendicular to it, then two different interpretations can be given. First, if $f=\sin$, then the scalar product
\[
\ket{a}\cdot\ket{b} = ab\sin\theta
\]
would correspond to the area of a parallelogram built on two vectors $\ket{a}$ and $\ket{b}$ or the area of a rectangle built on two vectors $\ket{a}$ and $\ket{b_\perp}$. This approach is fruitful and it is used in a more advanced version of the product of two vectors, related to \emph{tensor product} as discussed later.

The second simple option corresponds to $f=\cos$ and the scalar product
\[
\ket{a}\cdot\ket{b} = ab\sin\theta\,.
\]
It is the area of the rectangle built on two vectors $\ket{a}$ and $\ket{b_\parallel}$. 

\begin{mybio}{Fidelity, Alignment, and Overlap}
	Scalar product of unit vectors can be viewed as the measure of their "alignment" or "overlap". This interpretation is useful in quantum theory, where vectors are used to represent states of quantum systems. 
\end{mybio}

\subsection{Orthonormal Bases}
The choice of basis vectors is dictated by their usefulness in a given problem. In many cases it is convenient to use basis vectors which have unit length and which are \emph{mutually orthogonal}:
\[
\ket{u_1}\cdot\ket{u_1}=1=\ket{u_2}\cdot\ket{u_2}\,\textrm{ and }\,\ket{u_1}\cdot\ket{u_2}=0\,.
\]
In this case scalar product of any two vectors 
\[
\ket{a}=a_1\ket{u_1}+a_2\ket{u_2}\,\textrm{ and }\,\ket{b}=b_1\ket{u_1}+b_2\ket{u_2}\,.
\]
takes on a very simple form:
\[
\ket{a}\cdot\ket{b}=a_1b_1+a_2b_2\,.
\]
Although we arrived at this result analysing only two-dimensional case, it is easily generalized to any number of dimensions. For an $n$-dimensional case we would have the following expression for the scalar product
\[
\ket{a}\cdot\ket{b}=\int a_ib_i\,,\quad  i=\overline{1,n}\,.
\]


\section{Operators}\label{sec:operators}
Operators are functions of a particular kind. A "usual" function, like $\cos$, \emph{transforms} a number into another number, while an operator $\op{T}$ transforms a vector into another vector:
\[
\op{T}\,\ket{a} = \ket{b}\,.
\]
It is better to consider examples.
\begin{mybio}{Example Operators}\\
	It is easy to come up with examples of operators:
	
	\begin{itemize}
		\item\phantom{x}
		
		\item Unit operator (or \emph{identity} operator), such that
		\[
		\op{I}\,\ket{a}=\ket{a}\,.
		\]
		
		\item ``Zeroing'' operator that maps every vector into a zero
		vector:
		\[
		\op{0}\, \ket{a} = \ket{0}\,.
		\]
		
		\item ``Flipping'' operator that changes a vector into its opposite:
		vector:
		\[
		\op{F}\, \ket{a} = -\ket{a}\,.
		\]
		
		\item ``Orthogonal'' operator that makes any vector perpendicular to its original direction. In other words, it rotates the original vector by $90$ degrees counter-clockwise, leaving the length the same:
		\[
		\op{J}\, \ket{a} = \ket{b}\,,\textrm{ such that }\, b=a\,\textrm{ and }\,\ket{a}\cdot\ket{b}=0\,.
		\]
		Here $a$ and $b$ are the lengths of the vectors.
		
		\item ``Normalizing'' operator that turns an arbitrary vector into a normalized vector with the same direction:
		\[
		\op{N}\, \ket{a} = \ket{a}/a\,.
		\]
		
		\item Rotation operators that perform a rotation of a vector by a specified angle $\theta$:
		\[
		\op{R}_\theta\, \ket{a} = \ket{b}\,,\textrm{ such that }\, a=b\,,\textrm{ and } \,\ket{a}\cdot\ket{b}=a^2\cos\theta\,.
		\]
		Special cases of these operators are $\op{F}=\op{R}_\pi$, $\op{J}=\op{R}_{\pi/2}$ and $\op{I}=\op{R}_0$.
		
		\item Scaling operators that change the length of a given vector by a specified factor:
		\[
		\op{S}_x\, \ket{a} = x\ket{a}\,.
		\]
		Special cases of these operators are $\op{F}=\op{S}_{-1}$, $\op{I}=\op{S}_1$, and $\op{0}=\op{S}_0$.
		
	\end{itemize}
\end{mybio}

\subsection{Operator Algebra}
Since operators are functions, we can manipulate them like functions. In particular, we can use them as objects in their own right, which can be added or composed. Indeed, if we can add vectors, we can give useful meaning to the sum of operators:
\[
\op{T} = \op{I} + \op{J} + \op{S}_2\,.
\]
To fully describe an operator, we must find how it acts \emph{on any} vector. In this example we get
\[
\op{T}\,\ket{a} = \op{I}\,\ket{a} + \op{J}\,\ket{a} + \op{S}_2\,\ket{a}=\ket{a}+\ket{b}+2\ket{a}=3\ket{a}+\ket{b}\,,
\]
where the vector $\ket{b}$ is orthogonal to $\ket{a}$, while having the same length.

Operators can be multiplied by "usual" numbers, and they can be composed.


\subsection{Linear Operators}
Among the simplest non-trivial operators are \emph{linear operators}. As mentioned earlier, linear functions satisfy two conditions:
\[
\op{T}\,(\ket{a}+\ket{b}) = \op{T}\,\ket{a}+\op{T}\,\ket{b}\,,
\]
and
\[
\op{T}\,(x\ket{a}) = x\op{T}\,\ket{a}\,.
\]
Thus, operator action distributes over a sum, and numbers can be pulled outside.

To define a linear operator, we need only to describe how it affects basis vectors. Indeed, if an arbitrary vector $\ket{a}$ is represented in some basis, we can write
\[
\op{T}\,\ket{a} = \op{T}\,(a_1\ket{u_1}+a_2\ket{u_2}) = \op{T}\,(a_1\ket{u_1})+\op{T}\,(a_2\ket{u_2})\,,
\]
and then pull numbers outside to obtain:
\[
\op{T}\,\ket{a} = a_1\op{T}\,\ket{u_1}+a_2\op{T}\ket{u_2}\,.
\]
We only need to know $\ket{t_1}=\op{T}\,\ket{u_1}$ and $\ket{t_2}=\op{T}\ket{u_2}$, so that 
\[
\op{T}\,\ket{a}=a_1\ket{t_1}+a_2\ket{t_2}\,,\quad \op{T}\,\ket{b}=b_1\ket{t_1}+b_2\ket{t_2}\,,\textrm{ and so on}.
\]
Now both $\ket{t_1}$ and $\ket{t_2}$ are vectors and can be represented in the same basis as $\ket{a}$:
\[
\ket{t_1}=T_{11}\ket{u_1}+T_{12}\ket{u_2}\,,
\]
and
\[
\ket{t_2}=T_{21}\ket{u_1}+T_{22}\ket{u_2}\,.
\]
The four numbers $T_{11},\,T_{12},\,T_{21}$, and $T_{22}$ are called \emph{components of an operator} in a given basis. It is crucial to remember that these components are specific to a basis. If we decide to change to a different basis, components of all vectors and operators will change.

\begin{mybio}{Matrix}
	Components of a linear operator in a given basis are often written as square table, called \emph{matrix}:
	\[
	\op{T} =
	\begin{pmatrix}
		T_{11} & T_{12}\\
		T_{21} & T_{22}
	\end{pmatrix}\,.
	\]
	We emphasize again: If we change to a different basis, components of the operators will change, and the matrix will look different. Sometimes matrices are convenient for computations, but they are rarely used in this book.
\end{mybio}

\begin{example}
	Consider an operator that transforms the state $\ket{0}$ into a linear combination $\ket{+}=(\ket{0}+\ket{1})/\sqrt{2})$ and the state $\ket{1}$ into a linear combination $\ket{-}=(\ket{0}-\ket{1})/\sqrt{2})$.
	
	It is called \emph{Hadamard} operator and has the following matrix representation.
\end{example}

\begin{mybio}{Numbers On Steroids}
	Operators can be used to solve problems that do not have solutions in terms of real numbers. For example:
	\[
	\op{A}+\op{B} = 6\op{I}\,\textrm{ and }\, \op{A}\op{B}=36\op{I}\,.
	\]
	To simplify this problem we can first rescale the operators, introducing
	\[
	\op{a} = \op{A} / 6\,\textrm{ and }\, \op{b} = \op{B}/6\,.
	\]
	\[
	\op{a}+\op{b} = \op{I}\,\textrm{ and }\, \op{a}\op{b}=\op{I}\,.
	\]
	
\end{mybio}

\subsection{Super-operators*}
An idea of a function is quite general, it implies mapping one value to another, or calculting result given a certain input. Given a number, a function can yield another number, or given a vector a function can produce another vector. In the latter case we call the function an \emph{operator}.  

In the mathematical toolset of both classical and quantum physics there are functions of \emph{higher order} in the sense that they can act on functions or operators. For example,  a \emph{super-operator} is a mapping from any operator $\op{T}$ into another operator. Although it sounds abstract, it is not a complicated idea. Indeed, all we need is a rule that finds some operator $\op{B}$ given an operator $\op{A}$. One non-trivial rule can be stated as follows: Given an operator $\op{A}$, apply it twice:
\[
\op{A}\quad\overset{\mathcal{F}}{\longrightarrow}\quad\op{B}=\op{A}\circ\op{A}\,.
\]
Since the composition of two operators is again an operator, this is a satisfactory definition.


\section{Functionals}
Another important type of function is called \emph{functional}. A functional maps a function into a number. Let's consider several examples.

\begin{flushleft}
	{\it Total Mass}
\end{flushleft}
Suppose an astrophysicist is trying to model a spherically symmetric star and calculates \emph{density} of the star as the function of distance from its center: $r\rightarrow\rho_r$. The total mass of the star can then be evaluated as the sum of masses of all spherical shells with thickness $\delta r$:
\[
M = \int \delta V\rho_r=\int 4\pi r^2\delta r\rho_r\,.
\]
For a given function $\rho_r$ this summation will result in a number -- star's total mass. Such mapping $\rho_r\rightarrow M$ is an example of a functional.

\begin{flushleft}
	{\it Total Fuel}
\end{flushleft}
Consider a car moving on a straight highway between two points $A$ and $B$. The amount of fuel the engine consumes at a given moment depends on the speed of the car at that moment and can be described by the function $\mu_v$. Suppose the position of the car as the function of time $x_t$ is known and are looking for the total fuel consumed during the travel. This can be done in three steps. 

First, we find the speed of the car as the function of time by applying the operator $\partial_t$ to $x_t$: $v_t=\partial_{t}x$. Second, we find the fuel consuption rate $\mu$  as the function of time by plugging $v_t$ into $\mu_v$: $f_t = \mu(v_t)$. Finally, we can find the total amount of consumed fuel as the sum
\[
F = \int f_t\delta t\,.
\]
Combining all three steps into a single mathematical expression will result in a more cumbersome formula:
\[
F = \int \delta t\mu(\partial_t x)\,.
\]
This formula encodes a recipe for mapping any function $x_t$ into a number $F$ -- an example of a functional.

\begin{flushleft}
	{\it Total Action}
\end{flushleft}
A body in a "free fall" is moving with constant acceleration due to the force of gravity. Its speed increases as the body approaches the ground. If the body starts at rest at height $H$, its position along the vertical $y$ axis depends on time as $y_t=H-gt^2/2$ and the velocity changes according to the equation $v=-gt$.

The potenital energy $E_p=mgy$ of the body decreases, while its kinetic energy $E_k=mv^2/2$ grows. The total mechanical energy $E=E_p+E_k$ remains fixed according to the law of energy conservation. Thus, the potential energy of the body is transformed into the kinetic energy.

Another physical quantity is often important -- the \emph{imbalance} of kinetic energy over the potential energy:
\[
L = E_k - E_p\,.
\]
It does not remain constant, and for the case of a free fall we can easily find its time dependence:
\[
L_t = mg^2t^2 - mgH\,.
\]
Given $L_t$, we can calculate a fundamental physical quantity -- total \emph{action} of the process:
\[
A = \int\delta t L_t\,.
\]
The summation extends to the moment $t=T$ when the body reaches the ground ($y=0$). This happens at $T=\sqrt{2H/g}$.

Performing the summation requires evaluation of two familiar sums:
\[
\int t^2\delta t =\frac{T^3}{3}\quad\textrm{ and }\quad \int \delta t=T\,.
\]
Substituting the values of $T$ and simplifying, the expression for the total action takes the form
\[
A = mgT(\frac{gT^2}{3}-H)=-\frac{mH}{3}\sqrt{2gH}=-\frac{mv_{m}H}{3}\,.
\]
Here we used $v_m=gT=\sqrt{2Hg}$ -- the maximal speed of the body at the end of the free fall process. Finally, denoting the maximum momentum of the body as $p_m=mv_m$, we obtain $A=-p_m H/3$. Note that the action can be expressed as the product of momentum and distance.

\emph{Action} is a physical quantit of fundamental importance. It plays a prominent role in both classical mechanics (the principle of \emph{stationary action}) and in quantum physics (the principle of \emph{action quantization}). Both principles will be explored in details later in the book.

\begin{exercise}
	Calculate the total action of a free fall process for an electron falling from the height 0.1 meter.
\end{exercise}


\begin{flushleft}
	{\it Assorted Examples}
\end{flushleft}
Examples of functionals given above involve evaluation of sums in order to find  \emph{total quantities} of various kinds:
\[
Q = \int \delta x f_x\,.
\]
The total quantity $Q$ depends on the behavior of the input function $f_x$ over an extended range of $x$ values. Simpler forms of functionals can also be used. For example:
\[
\mathcal{M}\, f = f_0
\]
returns the value of the input function $f_x$ at zero. This functional, despite its trivial look, is very useful and widely used in physics and mathematics. Its rigorous mathematical form is called \emph{Dirac delta function}\,.
\begin{mybio}{Dirac Delta Function}
	The idea of delta function is simple: it describes the density of mass (or charge, probability, and so on) for a point-like particle. Formally, such density can be written as $\delta_x$.
	
	Since the total mass (charge, probability) is finite, the summation of the density over the region where the particle might be must be a fixed number:
	\[
	m = \int \delta x \delta_x\,.
	\]
\end{mybio}

Another example of a simple functional is the maximum of a function:
\[
\mathcal{X}\,f = \textrm{max}\,f_x\,.
\]
Finally, one can map any function $f_x$ into a number like so:
\[
\mathcal{R}\,f = \frac{f_1}{1!} + \frac{f_{1/2}}{2!} + \frac{f_{1/3}}{3!}+\ldots+\frac{f_{1/n}}{n!}+\ldots\,.
\]
For $f=\sin$ we obtain $\mathcal{R}\,\sin\approx 1.1479$.
\begin{exercise}
	For the functionals $\mathcal{M}$, $\mathcal{X}$, and $\mathcal{R}$ check whether they are \emph{linear}.
\end{exercise}

\section{Spaces}
In mathematics and physics the concept of \emph{space} becomes more abstract, in comparison with the intuitive view of space as a "container for things." Space in mathematical sense is more akin to how people understand space in expressions like "space of ideas", "space of solutions", "color space", "design space", "parameter space", and so on.

First of all, space is a set of mathematical objects of similar nature. For example, all numbers, or all arrows considered as one single entity, can be viewed as space. Second, unlike a simple set, space has some \emph{structure}, some \emph{relations} between its elements. For example, there might some sense of direction, or distance, or simply the notion of continuity. Advanced spaces, used in mathematical physics, have quite a complex structure, with distances, angles, algebraic operations, and even with operations required for calculus.

We will be mainly using \emph{vector spaces}, understood as rich collections of all vectors, endowed with addition and multiplication operations, as defined earlier.

\section{Duality}
Many mathematical concepts have very natural "companions", related in the spirit of mirror images or "yin-yang." In this kind of relationship both "companions" are on equal footing, neither concept is preferred, neither is basic while the other is derived. This situation is called \emph{duality}.

Scalar product of vectors leads to two interesting and useful dualities. The first duality deals with vectors and results in \emph{dual vectors}, while the second duality deals with operators and gives the notion of \emph{adjoint operators}.

\subsection{Dual Vectors}
To arrive at dual vectors we can start with scalar product, written using a \emph{prefix notation}:
\[
\lbrack\cdot\rbrack\,\ket{a}\,\ket{b} = x\,.
\]
Here we introduced a binary operator (function of two vector arguments) that maps a pair of vectors into a number.

The next step is to consider the operator $\lbrack\cdot\rbrack$ \emph{partially applied} to the first argument only:
\[
\lbrack\cdot\rbrack\,\ket{a}\tus\,.
\]
Although here we denoted "an empy slot" with a grey circle $\tus$, it is unnecessary and can be dropped. Furthermore, we will use Dirac notation for the mathematical object that results from the partial application of scalar product, writing it as follows:
\[
\bra{a} = \lbrack\cdot\rbrack\,\ket{a}\,.
\]
Such an object can be written for \emph{every} vector:
\[
\bra{b} = \lbrack\cdot\rbrack\,\ket{b}\,,\quad
\bra{c} = \lbrack\cdot\rbrack\,\ket{c}\,,\quad\textrm{ and so on.}
\]

A beautiful thing about this whole construction is that it gives us \emph{new kinds of vectors}. Also, it teaches us a lesson that vectors can be viewed as functions, revealing a connection between the two concepts.

It is easy to demonstrate that $\bra{a}$ is a linear function. Recall that a function with two arguments, when partially applied, becomes a function of a single argument. Therefore, since $\lbrack\cdot\rbrack$ is the function of two vector arguments, $\bra{a}$ must be the function of just one vector argument. Moreover, since $\lbrack\cdot\rbrack$ is linear in both arguments, $\bra{a}$ must be linear in its single argument. More explicitely:
\[
\bra{a}\,(\ket{b}+\ket{c})=\bra{a}\ket{b}+\bra{a}\ket{c}\,.
\]
To check this equality, we first replace $\bra{a}$ with its definition, then switch to the infix notation, and use the distributivity of scalar product.
\begin{exercise}
	Prove the equalities:
	\[
	\bra{a}\,(\ket{b}+\ket{c})=\bra{a}\ket{b}+\bra{a}\ket{c}\,,
	\]
	and
	\[
	\bra{a}\,(x\ket{b})=x\bra{a}\ket{b}\,.
	\]
\end{exercise}
\begin{mybio}{Bra, Ket, and Braket}
	In Dirac notation the application of $\bra{a}$ to $\ket{b}$ is written in a less noisy way:
	\[
	\braket{a}{b}\,.
	\]
	The whole expression is called \emph{bracket}, whereas $\bra{a}$ is called \emph{bra vector}, and $\ket{b}$ is called \emph{ket vector}.
\end{mybio}

Now how can we show that the collection of objects $\bra{a}$, $\bra{b}$, $\bra{c}$ and so on forms a vector space? To be a vector an object must behave like one, demonstrating all properties that fully define vector. We will examine whether bra-vectors can be added, and whether they can be expanded in some basis.

Can $\bra{a}$ and $\bra{b}$ be added? Of course they can, because they are functions over "addable things" (over vectors), and the meaning of $\bra{v}=\bra{a}+\bra{b}$ is simple:
\[
\braket{v}{c}=\braket{a}{c}+\braket{b}{c}\,.
\]
For similar reasons, any $\bra{a}$ can be multiplied by "usual" numbers, so that the expressions like
\[
\bra{w} = x\bra{a}+y\bra{b}
\]
have a straightforward interpretation.

Since there is exists a basis in the ket-vector space, there is one in the bra-space. Indeed, suppose we have $\ket{a}=a_1\ket{u_1}+a_2\ket{u_2}$ and we use the linearity of the scalar product in its first argument:
\[
\bra{a} = \lbrack\cdot\rbrack\,(a_1\ket{u_1}+a_2\ket{u_2})=a_1(\lbrack\cdot\rbrack\,\ket{u_1})+
a_2(\lbrack\cdot\rbrack\,\ket{u_2})\,.
\]
Since
\[
\lbrack\cdot\rbrack\,\ket{u_1} = \bra{u_1}\,,\quad\textrm{ and }\quad\lbrack\cdot\rbrack\,\ket{u_2} = \bra{u_2}\,,
\]
we conclude that any bra-vector can be expanded in terms of some other bra-vectors, just like ket-vectors are expanded in some basis:
\[
\bra{a} = a_1\bra{u_1}+a_2\bra{u_2}\,.
\]
In summary, if we need a bra-basis, we can use ket-basis and partially apply the scalar product operator to each ket-basis vector. 

One last step is needed to rigirously prove that both ket-vectors and bra-vectors are indeed vectors: we must investigate how their components change when we change bases. The only challenge this task presents is its tediousness (REF TO MY BOOK).

From now on we will recognize that to every ket-vector $\ket{a}$ there corresponds a \emph{dual} bra-vector $\bra{a}$. We must remember that this duality is generated by the operation of scalar product. Once a vector space has scalar product defined, we get a dual vector space "for free." Such dual vector space turns out extremely useful in quantum theory, as we will soon see.

\subsection{Dual Operators}
\begin{figure}[htbp]
	\centering
	\includegraphics[scale=1.0]{adjointOperator}
	\caption{Duality generated by the scalar product applies both the vector spaces and operators acting on them.}
	\label{fig:adjointOperator}
\end{figure}
We have shown that each vector $\ket{a}$ has dual $\bra{a}$, and the whole space of ket-vectors has a dual counterpart, as shown in Figure \ref{fig:adjointOperator}. Given an operator $\op{T}$ acting on ket-space, we can define an operator $\op{T}^*$ acting on bra-space. The action of  $\op{T}^*$ on an arbitrary bra-vector $\bra{a}$ is defined as follows: we first transform bra-vector $\bra{a}$ into its dual ket-vector $\ket{a}$ and find $\op{T}\ket{a}=\ket{c}$, and finally find its dual $\bra{c}$, by partially applying scalar product:
\[
\bra{c}=(\op{T}\ket{a})\cdot\tus\,.
\]
This procedure maps any $\bra{a}$ into some $\bra{c}$, as required. Let's examine how this bra-vector $\bra{c}$ acts on an arbitrary ket-vector $\ket{b}$:
\[
\braket{c}{b}=(\op{T}\ket{a})\cdot\ket{b}\,.
\]
\begin{mybio}{Postfix Notation}
	The action of an operator $\op{T}$ on a ket-vector $\ket{a}$ is nearly always written using \emph{prefix notation}:
	\[
	\op{T}\ket{a}=\ket{c}\,.
	\]
	To emphasize the distinction between dual vector spaces, the application of the dual operator $\op{T}^*$ on the dual vector $\bra{a}$ follows \emph{postfix notation}:
	\[
	\bra{a}\op{T}^*=\bra{c}\,.
	\]
	The use of prefix-postfix notations highlights the "mirror-like" relationship between the dual objects:
	\[
	\bra{a}\op{T}^*\,\,\Bigg\vert\,\,\op{T}\ket{a}
	\]
\end{mybio}

\begin{mybio}{Conjugation}
	Switching between dual objects -- vectors or operators -- is an important and often used operation in quantum theory. This operation is called \emph{conjugation} and is often denoted by an asterisk:
	\[
	\ket{a}\quad\overset{*}{\longrightarrow}\quad\bra{a}\,,
	\]
	\[
	\op{T}\quad\overset{*}{\longrightarrow}\quad\op{T}^*\,,
	\]
	\[
	\op{T}\ket{a}\quad\overset{*}{\longrightarrow}\quad\bra{a}\op{T}^*\,.
	\]
	Another, more traditional, way to express these relations is as follows:
	\[
	\bra{a} = (\ket{a})^*\,,\quad \textrm{ and }\quad \bra{a}\op{T}^*=(\op{T}\ket{a})^*\,.
	\]
\end{mybio}

\subsection{Adjoint Operators}
The notion of an \emph{adjoint} operator is very important in quantum theory. Before we define it, we'll explore the context in which adjoint operator is used.

To begin, recall that a linear operator transforms an input vector into an output vector. For some operators such transformation is always \emph{reversible}: There exist another operator that can "cancel" the action of the first, as illustrated in Figure \ref{fig:dualOperators}(a), where the operator $\op{U}$ acts as the \emph{inverse} of the operator $\op{T}$:
\[
\op{U}\circ\op{T}=\op{I}\,.
\]
The inverse of an operator, if there exists one, is denoted using the negative power: $\op{U}=\op{T}^{-1}$. Not every operator has an inverse, with $\op{0}$ ("zeroing" operator) and $\op{N}$ (normalizing operator) being simple examples.

Scalar product leads to more sophisticated relations between operators. One such relation, shown in Figure \ref{fig:dualOperators}(b), amounts to one operator being "inverse relative to the second input" of the scalar product. In simpler terms: \emph{Acting on both inputs changes nothing}. This can be expressed in the following equation:
\[
\ket{a}\cdot\ket{b}=(\op{T}\ket{a})\cdot(\op{U}\ket{b})\,.
\]
If this equality holds true for \emph{any} pair of vectors $\ket{a}$ and $\ket{b}$, then we will call the operator $\op{T}$ \emph{scalar-inverse} of the operator $\op{U}$. All rotation operators $\op{R}_\theta$ are scalar-inverse of themselves:
\[
\ket{a}\cdot\ket{b}=(\op{R}_\theta\ket{a})\cdot(\op{R}_\theta\ket{b})\,.
\]
Indeed, rotating both vectors by the same angle, does not change their mutual orientation and does not affect their lengths, keeping the product $ab\cos\theta$ constant.
\begin{exercise}
	Find the scalar-inverse of the scaling operator $\op{S}_x$.
\end{exercise}
\begin{figure}[htbp]
	\centering
	\includegraphics[scale=1.0]{dualOperators}
	\caption{Some operators have "relatives" which have certain interesting properties.}
	\label{fig:dualOperators}
\end{figure}

One more useful relation is generated by the scalar product. As illustrated in Figure \ref{fig:dualOperators}(c), this relation means that one operator, say $\op{T}$, achieves the same affect as the other operator (e.g. $\op{U}$), while acting on the second input of the binary scalar product operator $\lbrack\cdot\rbrack$. Symbollically we can write:
\[
\ket{a}\cdot(\op{U}\ket{b})=(\op{T}\ket{a})\cdot\ket{b}\,.
\]
If this is true for all possible vectors, then we will call the operator $\op{T}$ \emph{scalar-equivalent} to the operator $\op{U}$.  A conventional name for the scalar-equivalent operator is \emph{adjoint}. From the definition given above it is clear that both the original operator $\op{U}$ and its adjoint $\op{T}$ act on the same ket-vector space. The adjoint of the operator $\op{U}$ is often denoted using "\emph{dagger}"-notation:
\[
\op{T}=\op{U}^\dagger\,.
\]
\begin{mydef}{Adjoint Operator}
	An adjoint of the operator $\op{U}$ acting on ket-space, is another operator, denoted as $\op{U}^\dagger$, acting on the same ket-space, and satisfying the following requirement:
	\[
		\ket{a}\cdot(\op{U}\ket{b})=(\op{U}^\dagger\ket{a})\cdot\ket{b}
	\]
	\emph{for all} pairs of ket-vectors.
\end{mydef}

All scaling operators $\op{S}_x$ are scalar-equivalent to themselves:
\[
\ket{a}\cdot(\op{S}_x\ket{b})=(\op{S}_x\ket{a})\cdot\ket{b}\,.
\]
Indeed, scaling the first vector by some factor $x$ has the same effect on the product $ab\cos\theta$ as scaling the second vector by the same factor.
\begin{exercise}
	Find the scalar-equivalent of the rotation operator $\op{R}_\theta$.
\end{exercise}

Interestingly, finding the adjoint of a given operator is simpler than finding its inverse. This is most easily done using components:
\[
U_{ij} = \ket{u_i}\cdot(\op{U}\ket{u_j})=(\op{U}^\dagger\ket{u_i})\cdot\ket{u_j}\,,
\]
now we can use the commutativity of the scalar product
\[
U_{ij} = \ket{u_j}\cdot(\op{U}^\dagger\ket{u_i})=U^\dagger_{ji}\,.
\]
Thus, the components of the adjoint $U^\dagger_{ji}$ are obtained from the components of the operator $U_{ij}$ by "swapping" indices. For example:
\[
U^\dagger_{11} = U_{11}\,,U^\dagger_{22} = U_{22}\,,\quad\textrm{ and so on, while}
\]
\[
U^\dagger_{12} = U_{21}\,,U^\dagger_{23} = U_{32}\,,\quad\textrm{ and so on.}
\]
\begin{mybio}{Transposition}
	The swapping of components $T_{ij}\to T_{ji}$ is called \emph{transposition}, and the resultant operator whose components are $T_{ji}$ is called \emph{transposed} relative to the original operator $\op{T}$.
\end{mybio}

The concepts of conjugation and adjoint operator, discussed above, have interesting and powerful representations in terms of \emph{tensor product} -- the topic of the next section.


\section{Tensor Product}
A pair of vectors can be combined in a variety of ways, yielding results of different kinds, see Figure \ref{fig:vectorOperations}. 
Two familiar operations of vector addition and scalar product are close analogs of numeric addition and multiplication, respectively. Addition "glues" two vectors into the third vector, scalar product "fuses" two vectors into a number. There exists one more method of "bundling" vectors which results not in a number or yet another vector, but in a more advanced mathematical objects called \emph{tensor}. Tensors are closely related to operators and we will focus on this aspect of tensor product. 
\begin{figure}[htbp]
	\centering
	\includegraphics[scale=1.0]{vectorOperations}
	\caption{Three basic operation with vectors: a) Scalar product; b) vector sum; c) tensor product.}
	\label{fig:vectorOperations}
\end{figure}

We start with a pair of ket-vectors $\ket{a}$ and $\ket{b}$ and try "bundling" them in a way that resembles multiplication. Denoting this new operation $\otimes$, we want to have 
\[
\ket{a}\otimes\left(\ket{b}+\ket{c}\right)=\ket{a}\otimes\ket{b}+\ket{a}\otimes\ket{c}\,,
\]
and
\[
(x\ket{a})\otimes(y\ket{b})=xy\ket{a}\otimes\ket{b}\,.
\]
The last requirement indicates that this new type of product has the nature of an area, since it is proportional to the product of the lengths of two vectors. Thus, it can't be another vector, but must be a construction of different kind that we will call \emph{tensor}, and the product $\ket{a}\otimes\ket{b}$ -- a \emph{tensor product}. 

Using the arrow model of vectors, we can consider as a possible candidate a "blade" -- a piece of plane swept by "sliding" the vector $\ket{a}$ along the vector $\ket{b}$, as illustrated in Figure \ref{fig:vectorOperations}(c). This geometric object has "magnitude" or size, defined by the area of the blade, and it has direction or orientation. This approach is used in geometric algebra, where the \emph{outer product} is defined along these lines.

The geometric representation of product $\ket{a}\otimes\ket{b}$ might be appealing, but not at all required. Furthermore, it might even be unhelpful when we would like to generalize the operation to the product of other vector types:
\[
\ket{a}\otimes\bra{b}\,,\quad \bra{a}\otimes\ket{b}\,,\quad\textrm{ or }\quad\bra{a}\otimes\bra{a}\,.
\]
In other words, we should not constrain ourselves by reliance on a geometric intuition. How then do we interpret these various products? Well, one simple \emph{operational} criterion helps: A mathematical object is defined by \emph{what it does} and \emph{how it is used}. Let's study the two most important cases of tensor products.

\subsection{Ket-Ket}
The first tensor product is made of two ket-vectors:
\[
\op{P} = \ket{a}\otimes\ket{b}\,.
\]
In quantum theory, where ket-vectors "store" the information of all possible measurement (\emph{state} of a quantum system), the product is used to describe a \emph{joint system}, in which two distinct parts, say $A$ and $B$, can be identified and measured by different measuring devices.
\begin{figure}[htbp]
	\centering
	\includegraphics[scale=1.0]{ketketExample}
	\caption{Joint measurement of two systems.}
	\label{fig:ketketExample}
\end{figure}


Let's study an example, shown in Figure \ref{fig:ketketExample}. Imagine that we measure the value of some discrete quantity (quantized energy of an atom, for instance) using identical devices in different locations, corresponding to the systems $A$ and $B$. Suppose that in each trial the value of energy is measured to be either 1, or 2, or 3 \emph{randomly}. Then, after many trials of measuring the energy of the system $A$, we may summarize the statistics as follows:
\[
\ket{a}=a_1\ket{1}+a_2\ket{2}+a_3\ket{3}\,,
\]
where $a_i=N_i/N$ is the \emph{probability} to obtain the result $i$, expressed as the ratio of the number of times $N_i$ the value $i$ appeared in a series of $N$ measurements. Similarly, after many trials of measuring the energy of the system $B$, we may summarize the statistics as an expression:
\[
\ket{b}=b_1\ket{1}+b_2\ket{2}+b_3\ket{3}\,,
\]
with the similar meaning of the coefficients $b_i$.

Now we can write the summary of observation of two systems joined together:
\[
\op{P} = \ket{a}\otimes\ket{b}=a_1b_1 \ket{1}\otimes\ket{1}+a_1b_2 \ket{1}\otimes\ket{2}+\ldots a_3b_3 \ket{3}\otimes\ket{3}\,.
\]
From the probability theory it is known (see SECTION X) that the numbers $p_{ij}=a_ib_j$ give the probability to measure the value $\ket{i}$ in the experiment on the system $A$, while getting the value $\ket{j}$ in the measurement performed on $B$.

It might seem like unnecessary complication to use the tensor product expressions like
\[
\op{P} = \int p_{ij}\ket{i}\otimes\ket{j}
\]
instead of keeping separate vectors $\ket{a}$ and $\ket{b}$. But if we try to use individual vectors $\ket{a}$ and $\ket{b}$ when the results of joint measurements have the following statistics:
\[
\op{P}_0 = \frac{1}{2} \ket{1}\otimes\ket{1}+\frac{1}{2} \ket{2}\otimes\ket{2}\,,
\]
we discover that no $\ket{a}$ and $\ket{b}$ satisfy the requirement $\op{P}_0=\ket{a}\otimes\ket{b}$.
\begin{exercise}
	Prove that the assumption 
	\[
	\op{P}_0=\ket{a}\otimes\ket{b}
	\]
	leads to contraditions and, therefore, can not be true.	
\end{exercise}
The information encoded in the tensor product $\op{P}_0$ is an example of \emph{strongly correlated} behavior. In this example, it says that whenever system $A$ is found (randomly!) with the energy $1$, the same energy is measured on the system $B$; similarly for the energy $2$. Effects like these are of great interest in quantum theory, and the operation of tensor product is indispensable for describing such phenomena.


\subsection{Ket-Bra}
The second type of tensor product involves one ket-vector and one bra-vector, "bundled" into a new object which behaves like an \emph{operator}, unlike braket used in scalar product. Given vectors $\ket{a}$ and $\bra{b}$, we can form a tensor product
\[
\op{P}=\ket{a}\otimes\bra{b}\,.
\] 
\begin{mybio}{Economical Notation}
	Instead of writing $\ket{a}\otimes\ket{b}$ we will often write $\ket{a}\ket{b}$. Instead of $\ket{a}\otimes\bra{b}$ we will often write $\ket{a}\bra{b}$.
\end{mybio}
What does $\op{P}$ do? It can be used as an operator, acting on ket-vectors as follows:
\[
\op{P}\ket{c}=\ket{a}\otimes\bra{b}\ket{c}=\ket{a}\braket{b}{c}=x\ket{a}\,,
\]
where $x=\braket{b}{c}$ is a number. The operator $\ket{a}\otimes\bra{b}$ transforms any input vector into a vector parallel to $\ket{a}$. In other words, it \emph{projects} all vectors onto the direction specified by $\ket{a}$. Operators of this kind are sometimes called \emph{projectors}, but we will reserve this name for the operators of special form $\ket{a}\otimes\bra{a}$.
\begin{exercise}
	Suppose $\ket{a}\perp \ket{b}$ and $\op{P}=\ket{a}\otimes\bra{b}$. Find the result of $\op{P}\ket{a}$ .
\end{exercise}
\begin{mydef}{Projector}
	A tensor product $\ket{a}\otimes\bra{a}$ is called \emph{projector}. It projects all vectors onto the direction specified by the vectors $\ket{a}$, while only scaling $\ket{a}$:
	\[
	\ket{a}\otimes\bra{a}\ket{a}=a^2\ket{a}\,.
	\]
\end{mydef}

In quantum theory ket-vectors represent the state of a quantum system -- the information about all possible measurement results. Projectors like $\ket{a}\otimes\bra{b}$ describe the change of state $\ket{b}\to\ket{a}$, or \emph{transition} between states. 

Another application of tensor products in quantum theory is representation of \emph{operators} in a choses basis. To see this, recall that every linear operator is fully specified once we know how it transforms basis vectors:
\[
\op{T}\ket{a}=\op{T}(a_1\ket{u_1}+a_2\ket{u_2})=a_1(\op{T}\ket{u_1})+a_2(\op{T}\ket{u_2})\,.
\]
If $\op{T}\ket{u_1}=\ket{t_1}$, then we can express it using tensor product $\op{P}_1=\ket{t_1}\otimes\bra{u_1}$. The transormation of $\ket{u_2}$ can be written similarly: $\op{P}_2=\ket{t_2}\otimes\bra{u_2}$. Since $\op{P}_1$ and $\op{P}_2$ are both operators, they can be multiplied by numbers and added to obtain another operator. The action of $\op{T}$ then becomes expressed as
\[
\op{T}\ket{a}=a_1\op{P}_1\ket{u_1}+a_2\op{P}_2\ket{u_2}=\op{P}_1(a_1\ket{u_1})+\op{P}_2(a_2\ket{u_2})\,.
\] 
Now since 
\[
\op{P}_1(a_1\ket{u_1})=\op{P}_1(a_1\ket{u_1}+a_2\ket{u_2})=\op{P}_1 \ket{a}\,,
\]
and
\[
\op{P}_2(a_2\ket{u_2})=\op{P}_2(a_1\ket{u_1}+a_2\ket{u_2})=\op{P}_2 \ket{a}\,,
\]
because of the orthogonality of the basis. For example:
\[
	\op{P}_1(a_2\ket{u_2})=a_2\ket{t_1}\otimes\bra{u_1}\ket{u_2}=0\,.
\]

The action of the operator $\op{T}$ becomes
\[
\op{T}\ket{a}=\op{P}_1\ket{a}+\op{P}_2\ket{a}=\left(\op{P}_1+\op{P}_2\right)\ket{a}\,.
\]
Now we can drop the input ket-vector, writing the operator in terms of the ket-bra tensor products:
\[
\op{T}=\ket{t_1}\otimes\bra{u_1}+\ket{t_2}\otimes\bra{u_2}\,.
\]
We can leave the expression as is or convert it into tensor product of only basis vectors. Recalling that 
\[
\ket{t_1}=T_{11}\ket{u_1}+T_{12}\ket{u_2}\,,\quad\textrm{ and }\quad \ket{t_2}=T_{21}\ket{u_1}+T_{22}\ket{u_2}\,,
\]
and using the linearity of the tensor product, we arrive at the following exression:
\[
\op{T}=T_{11}\ket{u_1}\bra{u_1}+T_{12}\ket{u_1}\bra{u_2}+
T_{21}\ket{u_2}\bra{u_1}+T_{22}\ket{u_2}\bra{u_2}\,.
\]
Here we used the \emph{economical notation} for tensor product. It can be further compactified and generalized to any number of dimensions:
\[
\op{T}=\int T_{ij}\ket{u_i}\bra{u_j}\,,\quad i,j=\overline{1,n}\,.
\]
\subsection{Useful Ket-Bras}
A unit operator $\op{I}$ plays an important role in many calculations. It can be represented as 
\[
\op{I}=\int\ketbra{u_i}{u_i}\,,\quad i=\overline{1,n}\,.
\]
It is easy to check this for $n=2$ explicitely:
\[
(\ketbra{u_1}{u_1}+\ketbra{u_1}{u_1})\ket{a}=\ket{u_1}\braket{u_1}{a}+\ket{u_2}\braket{u_2}{a}\,.
\]
The right-hand side is clearly the expansion of the ket-vector $\ket{a}$ in the basis $\ket{u_i}$:
\[
\ket{u_1}\braket{u_1}{a}+\ket{u_2}\braket{u_2}{a}=a_1\ket{u_1}+a_2\ket{u_2}=\ket{a}\,.
\]
\begin{mybio}{Completeness}
	The expression
	\[
	\op{I}=\int\ketbra{u_i}{u_i}\,,\quad i=\overline{1,n}\,
	\]
	is called the \emph{completeness} statement of the set of basis vectors. It says that the set of orthonormal vectors is \emph{not missing} any vector necessary to construct all other vectors in the space. Indeed, if we omit even a single term $\ketbra{u_k}{u_k}$ in the sum $\int\ketbra{u_i}{u_i}$ then the term $a_k\ket{u_k}$ will be missing in the expansion
	\[
	\left(\int\ketbra{u_i}{u_i}\right)\ket{a}=a_1\ket{u_1}+\ldots+a_{k-1}\ket{u_{k-1}}+a_{k+1}\ket{u_{k+1}}+\ldots a_n\ket{u_n}\,.
	\]
\end{mybio}

The operator $\op{J}$ that rotates vectors in a plane by 90 degrees counter-clockwise takes the basis ket-vector $\ket{u_1}$ into $\ket{u_2}$ and $\ket{u_2}$ into $(-\ket{u_1})$. Using tensor products $\op{J}$ can be written as follows:
\[
\op{J}=\ketbra{u_2}{u_1}-\ketbra{u_1}{u_2}\,.
\]


\subsection{Tensor Product of Operators}


\begin{exercise}
	Write Hadamard operator in terms of the tensor product of vectors $\ket{0}$, $\ket{1}$, $\ket{+}$ and $\ket{-}$.
	
	\[
	\op{H} = \ketbra{+}{0}+\ketbra{-}{1}\,.
	\]
\end{exercise}

\section{Functions As Vectors}
Perhaps surprisingly, but certain classes of functions, which are very useful in physics, have all characteristic of vectors. 

\section{Application: Circular Motion}
Let us examine how the concepts and tools discussed above can be applied to a simple case of circular motion.  

Consider a  particle moving in a circle with the radius $R$, as shown in Figure X. If we choose the center of the circle as the reference point, we can specify the position of the particle using an arrow $\ket{r}$. During motion the direction of this arrow is constantly changing, but its length $R$ remains the same.  

After a short time interval $\delta t$, the position of the particle changes by $\delta\ket{r}$:
\[
\ket{r_t}\quad\rightarrow\quad \ket{r_{t+\delta t}} = \ket{r_t}+\delta\ket{r}\,.
\]

The length of the path covered by the particle during the time interval $\delta t$ can be approximated by the length of the arc  $\delta L=R\delta\theta=v\delta t$. The arrow $\delta\ket{r}$ can be written as $\delta L\ket{u}$ where $\ket{u}$ is the vector of unit length pointing in the direction of motion. This unit vector can be constructed from $\ket{r}$ by scaling it down by $R$ and then rotating counter-clockwise with the operator $\op{J}$: 
\[
\delta\ket{r}=R\delta\theta\op{J}\left(\frac{\ket{r}}{R}\right)\,.
\]
Since $\op{J}$ is a linear operator, the $R$ cancels and we can write
\[
\frac{\delta\ket{r}}{\delta t}=\frac{\delta\theta}{\delta t}\op{J}\ket{r}\qquad\Longrightarrow\qquad
\partial_t\ket{r}=\omega\op{J}\ket{r}\,,
\]
where we introduced the angular speed $\omega=\partial_t\theta$. Finally, by applying the $\op{J}$ operator to both sides of the last equation, we can cast it into the "Schrodinger" form:
\[
\op{J}\partial_t\ket{r}=-\omega\ket{r}\,.
\]  




\section*{Chapter Highlights}
{\setstretch{1.5}\chhc
  \it
\begin{itemize}
\item Arrows in a plane provide a simple model for vectors.
\item Arrows can be manipulated in ways analogous to numbers: Two arrows
  be added, an arrow can be ``scaled'' (stretched or compressed). Arrows form
  an algebra.
\item Basis is an extremely important concept. Basis is a set of
  objects (arrows) that can be used to ``build'' all other similar
  objects (arrows). At the same time, basis can not be used to build
  itself -- basis arrows are independent.
\end{itemize}
}
